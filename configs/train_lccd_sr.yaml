resume: /public_bme/data/linxin_debug/Loss/_train_lccd_sr_batch_size_2/20241025_151214/epoch-last.pth
ask_user: false  # 是否询问用户是否覆盖已有文件
remove_save_path: false  # 是否删除已有文件

train_dataset:
  dataset:
    name: paired-image-folders
    args:
      root_path_1: /home_data/home/linxin2024/code/3DMedDM_v2/data_try_lx/train/train_T1_img.txt
      root_path_2: /home_data/home/linxin2024/code/3DMedDM_v2/data_try_lx/train/train_SWI_img.txt
      prompt_D1_M1: /home_data/home/linxin2024/code/3DMedDM_v2/data_try_lx/train/train_T1_prompt.txt
      prompt_D1_M2: /home_data/home/linxin2024/code/3DMedDM_v2/data_try_lx/train/train_SWI_prompt.txt
      repeat: 1
      cache: in_memory
  wrapper:
    name: sr-implicit-paired
    args:
      # inp_size: 60
#      scale_min: 1
#      scale_max: 3
      augment: true # 数据增强
      sample_q: 8000
  batch_size: 2

val_dataset:
  dataset:
    name: paired-image-folders
    args:
      root_path_1: /home_data/home/linxin2024/code/3DMedDM_v2/data_try_lx/train/train_T1_img.txt
      root_path_2: /home_data/home/linxin2024/code/3DMedDM_v2/data_try_lx/train/train_SWI_img.txt
      prompt_D1_M1: /home_data/home/linxin2024/code/3DMedDM_v2/data_try_lx/train/train_T1_prompt.txt
      prompt_D1_M2: /home_data/home/linxin2024/code/3DMedDM_v2/data_try_lx/train/train_SWI_prompt.txt
      repeat: 1
      cache: in_memory
  wrapper:
    name: sr-implicit-paired
    args:
      #inp_size:
#      scale_min: 1
#      scale_max: 3
      augment: true
      sample_q: 8000
  batch_size: 2


model_G:
  name: lccd
  args:
    encoder_spec:
      name: resencoder-256
      args:
        no_upsampling: true
        #scale: 1
    no_imnet: False

model_D:
  name: NLDiscri
  args:
    in_dim: 8000
    out_dim: 864
    hidden_list: [256, 256, 256, 256]

optimizer_G: # 生成器
  name: adam
  args:
    lr: 1.e-4
optimizer_D: # 判别器
  name: adam
  args:
    lr: 1.e-4

epoch_max: 500
multi_step_lr:
  milestones: [100, 220, 300, 350, 400]
  gamma: 0.5


epoch_val: 10
epoch_save: 10

# Add training parameters
training_params:
  patch_size: [32, 32, 32]
  overlap_ratio: 0.25
  patch_batch_size: 32